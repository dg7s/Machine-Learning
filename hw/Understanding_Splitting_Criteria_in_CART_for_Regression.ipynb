{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6 - Tree-based Methods\n",
        "\n",
        "Dominik Gaweł\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dg7s/Machine-Learning/blob/main/hw/Understanding_Splitting_Criteria_in_CART_for_Regression.ipynb)\n",
        "-------------------------------"
      ],
      "metadata": {
        "id": "uqNdtiN63fOJ"
      },
      "id": "uqNdtiN63fOJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Understanding Splitting Criteria in CART for Regression**\n",
        "---------------------\n",
        "\n",
        "In this assignment, you will explore three common formulations of the splitting criterion used in **CART (Classification and Regression Trees)** for **regression problems**:\n",
        "\n",
        "1. **Local RSS Minimization**  \n",
        "2. **RSS Gain Maximization**  \n",
        "3. **Total RSS Minimization**\n",
        "\n",
        "You will investigate whether any of these criteria are equivalent, and you will design an experiment to determine which criterion is actually employed in a standard implementation such as **scikit-learn’s DecisionTreeRegressor**.\n",
        "\n",
        "\n",
        "\n",
        "## **The Problem**\n",
        "\n",
        "Many treatments of CART for regression describe the split selection process in different ways. Below are three frequently cited formulations. Suppose we have a dataset with features $X$ and target $y$, and we seek to choose a feature $X_j$ and a threshold $t$ to split the data into two regions $R_1(X_j, t)$ and $R_2(X_j, t)$. Denote by $\\bar{y}_{R_m}$ the mean of targets within region $R_m$.\n",
        "\n",
        "1. **Local RSS Minimization**  \n",
        "   We select the feature and threshold that minimize the **sum of squared errors** in the two resulting child nodes:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\min_{X_j, t} \\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2.\n",
        "   $$\n",
        "\n",
        "2. **RSS Gain Maximization**  \n",
        "\n",
        "   It is also a local method, looking only at a parent and two child nodes.\n",
        "\n",
        "   We select the feature and threshold that maximize the **reduction** in RSS, computed by subtracting the RSS of the two child nodes from the RSS in the parent node:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\max_{X_j, t} \\Bigl\\{\n",
        "   \\underbrace{\\sum_{i : x_i \\in \\text{Parent}} (y_i - \\bar{y})^2}_{\\text{Parent RSS}}\n",
        "   \\;-\\;\n",
        "   \\underbrace{\\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2}_{\\text{Children RSS}}\n",
        "   \\Bigr\\}.\n",
        "   $$\n",
        "\n",
        "3. **Total RSS Minimization**  \n",
        "   For a dataset $\\{(x_i, y_i)\\}_{i=1}^N$ with features $X$ and target $y$, let $T$ be the current tree.\n",
        "\n",
        "   For any split on feature $X_j$ at threshold $t$, define $T(X_j, t)$ as the new tree obtained by splitting one leaf of $T$ into two leaves $R_1(X_j, t)$ and $R_2(X_j, t)$.\n",
        "   \n",
        "   Let $\\mathrm{Leaves}(T(X_j, t))$ be the set of all leaf indices in this new tree. For each leaf $m \\in \\mathrm{Leaves}(T(X_j, t))$, define:\n",
        "   $$\n",
        "   R_m = \\{\\, i \\,\\mid\\, x_i \\text{ ends in leaf } m\\}.\n",
        "   $$\n",
        "\n",
        "   $R_m$ set collects all data indices $i$ whose feature vector $x_i$ is classified into the leaf node $m$ when passed through the tree $T(X_j,t)$. In other words, each leaf node $m$ in $T(X_j, t)$ corresponds to a unique path of splits, and any data point $x_i$ that follows that path is assigned to the leaf $m$; hence, it belongs to $R_m$.\n",
        "\n",
        "   $R_m$ sets for all leafs $m \\in \\mathrm{Leaves}(T(X_j, t))$ define a partition of all indices.\n",
        "\n",
        "   Then the objective of **minimizing total Residual Sum of Squares (total RSS)** is stated as:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\min_{(X_j, t)} \\sum_{m \\in \\mathrm{Leaves}(T(X_j, t))}\n",
        "   \\sum_{i \\in R_m} \\Bigl(y_i - \\overline{y}_{R_m}\\Bigr)^2,\n",
        "   $$\n",
        "   where\n",
        "   $$\n",
        "   \\overline{y}_{R_m} = \\frac{1}{\\lvert R_m \\rvert}\n",
        "   \\sum_{i \\in R_m} y_i\n",
        "   $$\n",
        "   is the mean response in leaf $m$.\n",
        "\n",
        "\n",
        "## **Research Questions**\n",
        "\n",
        "1. **Equivalence Analysis**  \n",
        "   Determine whether the above formulations are equivalent or if they can yield different split choices. Specifically:\n",
        "   - Are *local RSS minimization* and *RSS gain maximization* equivalent?\n",
        "   - Does *total RSS minimization* coincide with either of these two, or is it distinct?\n",
        "   \n",
        "2. **Empirical Experiment**  \n",
        "   Design and conduct a Python experiment to determine which of these formulations is implemented in `scikit-learn` in `DecisionTreeRegressor`. Present numerical results and plots to support your conclusion.\n",
        "\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "1. **Formulation Analysis**  \n",
        "   - Compare *local RSS minimization*, *RSS gain maximization*, and *total RSS minimization*.\n",
        "   - If you find that any pair of formulations is equivalent, provide a concise proof.  \n",
        "   - If you find that they differ, construct a counterexample.\n",
        "\n",
        "2. **Empirical Verification**  \n",
        "   - Create a small artificial dataset and train a `DecisionTreeRegressor` from `scikit-learn`.\n",
        "   - The dataset must be designed in a way that uniquely identifies the formulation used. Provide a short code snippet and a plot or table to support your conclusion.\n",
        "\n",
        "3. **Report**  \n",
        "   - Summarize your theoretical insights and empirical findings in a **Colab notebook**.\n",
        "   - Include the relevant proofs, code, discussion, and conclusions.\n",
        "   - Place the notebook in your **GitHub repository** for this course, add a link to it in your README.md and add an **“Open in Colab”** badge in the notebook so it can be launched directly.\n",
        "\n"
      ],
      "metadata": {
        "id": "CiiBWLwyz6PI"
      },
      "id": "CiiBWLwyz6PI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equivalence Analysis\n",
        "\n",
        "### 1. Local RSS Minimization vs. RSS Gain Maximization for a single node\n",
        "For a single node (i.e. a local split), the two formulations are equivalent:\n",
        "\n",
        "\\begin{equation}\n",
        "\\arg\\max_{X_j, t} \\Bigl\\{\n",
        "   \\underbrace{\\sum_{i : x_i \\in \\text{Parent}} (y_i - \\bar{y})^2}_{\\text{Parent RSS}}\n",
        "   \\;-\\;\n",
        "   \\underbrace{\\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2}_{\\text{Children RSS}}\n",
        "   \\Bigr\\} =  \\arg\\min_{X_j, t} \\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2.\n",
        "\\end{equation}\n",
        "Since the **Parent RSS** is constant for all candidate splits, maximizing the reduction is equivalent to directly minimizing the **children's RSS**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Local RSS Minimization vs. RSS Gain Maximization\n",
        "\n",
        "When building a tree partially—enforced by, for example, setting `max_leaf_nodes`—we do not allow the tree to grow until every leaf is perfectly split. This restriction is necessary in order to highlight differences between the local splitting methods and the global RSS minimization.\n",
        "\n",
        "Consider the following one-dimensional dataset:\n",
        "\n",
        "$$\n",
        "X = (0,\\,1,\\,2,\\,3,\\,4), \\quad Y = (0,\\,1,\\,10,\\,11,\\,12).\n",
        "$$\n",
        "\n",
        "We have a first split at $X > 1.5$. This results in:\n",
        "\n",
        "- **Region 1:**  \n",
        "  $$ R_1 = \\{X_1 = (0,\\,1), \\quad Y_1 = (0,\\,1)\\} $$  \n",
        "  $$ \\bar{y}_{R_1} = 0.5,\\quad RSS_{R_1} = 0.5 $$\n",
        "- **Region 2:**  \n",
        "  $$ R_2 = \\{X_2 = (2,\\,3,\\,4), \\quad Y_2 = (10,\\,11,\\,12)\\} $$  \n",
        "  $$ \\bar{y}_{R_2}  = 11,\\quad RSS_{R_2} = 2. $$\n",
        "\n",
        "Now, consider further splitting one of these regions:\n",
        "- **Splitting Region 1:**With only two observations, we can perfectly split it, reducing its RSS from $0.5$ to $0$.\n",
        "- **Splitting Region 2:** The best split may only reduce its RSS from $2$ to $0.5$.\n",
        "\n",
        "**Local RSS minimization** selects the split with the lowest post-split RSS, so Region 1 is optimal (RSS becomes $0$ instead of $0.5$). In contrast, **RSS Gain Maximization** chooses the split with the largest reduction in RSS, favoring Region 2 (gain of $1.5$ versus $0.5$). This illustrates that the methods are not equivalent.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Equivalence of RSS Gain Maximization and Total RSS Minimization\n",
        "\n",
        "\n",
        "Now we will show that **RSS Gain Maximization** and **Total RSS Minimization** are equivalent. Notice that the difference in Total RSS between the given tree and the tree after a split is given by\n",
        "\n",
        "$$\n",
        "\\Delta RSS = \\text{RSS of the chosen leaf} - \\text{(sum of RSS of the children of the chosen leaf)}.\n",
        "$$\n",
        "\n",
        "By selecting the leaf whose optimal split minimizes the new Total RSS, we effectively choose the one that produces the largest $\\Delta RSS$. In other words, minimizing the Total RSS is equivalent to maximizing the RSS gain. Therefore, the two formulations are equivalent."
      ],
      "metadata": {
        "id": "jv5xCMnG4wKV"
      },
      "id": "jv5xCMnG4wKV"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor, export_text\n",
        "\n",
        "X = np.array([[0], [1], [2], [3], [4]])\n",
        "y = np.array([0, 1, 10, 11, 12])\n",
        "\n",
        "reg = DecisionTreeRegressor(max_leaf_nodes=3)\n",
        "reg.fit(X, y)\n",
        "\n",
        "tree_rules = export_text(reg, feature_names=[\"X\"])\n",
        "print(tree_rules)"
      ],
      "metadata": {
        "id": "teiF-aWX4wUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6bfb60d-a974-437f-99a0-0cd052ecf42d"
      },
      "id": "teiF-aWX4wUW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|--- X <= 1.50\n",
            "|   |--- value: [0.50]\n",
            "|--- X >  1.50\n",
            "|   |--- X <= 2.50\n",
            "|   |   |--- value: [10.00]\n",
            "|   |--- X >  2.50\n",
            "|   |   |--- value: [11.50]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn uses **RSS gain maximization**, as it chooses to split Region 2 to achieve the largest reduction in total RSS."
      ],
      "metadata": {
        "id": "O1W9udId2nCz"
      },
      "id": "O1W9udId2nCz"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}